{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'intermediates', 'main.ipynb', 'logs', 'checkpoints', 'input', 'data', 'two_layer_gruintrpd.h5']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "physical_devices = tensorflow.config.list_physical_devices('GPU')\n",
    "tensorflow.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "DATA = 'input/'\n",
    "INTERMEDIATES = 'intermediates/'\n",
    "MY_NAME = 'Aayush Fadia'\n",
    "OUTFILE='all_chats.txt'\n",
    "MODEL_SAVENAME='two_layer_gru'\n",
    "print(os.listdir('.'))\n",
    "if DATA[:-1] not in os.listdir('./'):\n",
    "    os.mkdir(DATA)\n",
    "if INTERMEDIATES[:-1] not in os.listdir('./'):\n",
    "    os.mkdir(INTERMEDIATES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "chats = os.listdir(DATA)\n",
    "def preproocess_message(msg):\n",
    "    msg = re.sub(r'(\\w)\\1+',r'\\1', msg)\n",
    "    msg = re.sub('\\d\\w+', '#', msg)\n",
    "    msg = re.sub('\\d', '#', msg)\n",
    "    msg = re.sub('this mesage was deleted', '[DELETED_MESSAGE]', msg)\n",
    "    msg = re.sub('media omited', '[MEDIA]', msg)\n",
    "    msg = msg.encode('ascii', 'ignore').decode('ascii').strip()+' '\n",
    "    return msg\n",
    "with open(INTERMEDIATES+OUTFILE, 'w') as outfile:\n",
    "    for chat in chats:\n",
    "        outfile.write('[SOC] ')\n",
    "        with open(DATA+chat, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                try:\n",
    "                    msgr = line.split(':')[1].split('-')[1].strip()\n",
    "                    sender_token = '[ME] ' if msgr==MY_NAME else '[THEM] '\n",
    "                    msg = line.split(':')[2].strip().lower().translate(translator)\n",
    "                    msg = preproocess_message(msg)\n",
    "                    outfile.write(sender_token+msg)\n",
    "                except IndexError:\n",
    "                    pass\n",
    "        outfile.write('[EOC] ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 137894\n",
      "Number of Distinct tokens: 8426\n"
     ]
    }
   ],
   "source": [
    "all_chats_file = open(INTERMEDIATES+OUTFILE, 'r')\n",
    "all_chats = all_chats_file.readlines()[0]\n",
    "all_chats = re.sub(' +', ' ',all_chats)\n",
    "all_chats_words = all_chats.split(' ')\n",
    "print(\"Number of tokens: \"+str(len(all_chats_words)))\n",
    "vocab = list(set(all_chats_words))\n",
    "print(\"Number of Distinct tokens: \"+str(len(vocab)))\n",
    "totalsize = len(vocab)\n",
    "token2word = dict()\n",
    "word2token = dict()\n",
    "for i in range(len(vocab)):\n",
    "    token2word[i] = vocab[i]\n",
    "    word2token[vocab[i]] = i\n",
    "all_chats_tokens = [word2token[word] for word in all_chats_words]\n",
    "all_chats_tokens_np = np.asarray(all_chats_tokens, np.uint16)\n",
    "del all_chats_words\n",
    "all_chats_file.close()\n",
    "del all_chats\n",
    "del vocab\n",
    "del all_chats_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = tensorflow.data.Dataset.from_tensor_slices(all_chats_tokens_np)\n",
    "sequences = full_dataset.batch(33, drop_remainder=True)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU, Input\n",
    "tf.backend.clear_session()\n",
    "def loss(labels, logits):\n",
    "  return tf.losses.sparse_categorical_crossentropy(labels, logits)\n",
    "model = Sequential()\n",
    "model.add(Embedding(totalsize, 256))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(Dense(totalsize, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.backend.clear_session()\n",
    "model = tf.models.load_model('checkpoints/'+MODEL_SAVENAME+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging_callback = tf.callbacks.TensorBoard(log_dir='logs/'+MODEL_SAVENAME, update_freq=1000, profile_batch=0)\n",
    "checkpoint_callback = tf.callbacks.ModelCheckpoint('checkpoints/'+MODEL_SAVENAME+'.h5', save_freq=200000)\n",
    "while True:\n",
    "    try:\n",
    "        model.fit(dataset, epochs=10000, callbacks=[logging_callback, checkpoint_callback], verbose=False)\n",
    "    except KeyboardInterrupt:\n",
    "        model.save(MODEL_SAVENAME+'intrpd.h5')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tensorflow import get_logger\n",
    "get_logger().setLevel(logging.ERROR)\n",
    "from tensorflow import convert_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tere samne \n",
      "[THEM] nothing \n",
      "[ME] say hey at the very least \n",
      "[ME] send him this \n",
      "[THEM] nopei wont do it \n",
      "[THEM] il try to get over him \n",
      "[ME] incoming \n",
      "[ME] [MEDIA] \n",
      "[ME] !!GENERATION BEGINS!!\n",
      "\n",
      "i am like a problem with you \n",
      "[THEM] fineish kaise karna \n",
      "[ME] oh shit \n",
      "[THEM] i dont think so \n",
      "[ME] i dont think so \n",
      "[ME] you deleted this mesage \n",
      "[ME] i dont think so \n",
      "[ME] i dont think so \n",
      "[ME] i dont think so \n",
      "[ME] i dont think so \n",
      "[ME] "
     ]
    }
   ],
   "source": [
    "ip = dataset.take(1)\n",
    "for x, _ in ip:\n",
    "    x = x[0]\n",
    "    for tkn in x:\n",
    "        tkn = tkn.numpy()\n",
    "        word = token2word[tkn]\n",
    "        if word in ['[THEM]', '[ME]']:\n",
    "            print()\n",
    "        print(word, end=' ')\n",
    "    print('!!GENERATION BEGINS!!')\n",
    "    print()\n",
    "    for _ in range(200):\n",
    "        y_pred = model.predict(convert_to_tensor(np.asarray([x])))\n",
    "        y_pred = y_pred[0][-1]\n",
    "        y_pred = np.argsort(y_pred)[::-1]\n",
    "        if token2word[y_pred[0]] in ['[THEM]', '[ME]']:\n",
    "            print()\n",
    "        print(token2word[y_pred[0]], end=' ')    \n",
    "        x = np.append(x, y_pred[0])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(msg):\n",
    "    msg = preproocess_message(msg).strip()\n",
    "    tokens = []\n",
    "    for word in msg.split(' '):\n",
    "        try:\n",
    "            tokens.append(word2token[word])\n",
    "        except KeyError:\n",
    "            print(word+' is out of vocabulary')\n",
    "    return np.asarray(tokens, dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[ME] hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[THEM] tuje mei use kar diya \n",
      "[THEM] nai \n",
      "[THEM] ab ko bhi hi nai tha "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[ME] jo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hum hai \n",
      "[THEM] nai \n",
      "[THEM] yeh kb nai aj \n",
      "[THEM] "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[ME] yeh kab hua\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[THEM] "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[ME] exam\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[THEM] # bhi \n",
      "[THEM] ha "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[ME] aaj bhi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nahi hai \n",
      "[THEM] yup "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[ME] lecture kitne baje hai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "na pls \n",
      "[THEM] \n",
      "[THEM] aj bhi nai h "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[ME] dude what is up\n",
      "[ME] with you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the first year \n",
      "[THEM] you told me to you \n",
      "[THEM] i have to come \n",
      "[THEM] # \n",
      "[THEM] you wana go at # "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[ME] okay, 6 is fine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay, is out of vocabulary\n",
      "with you \n",
      "[THEM] \n",
      "[THEM] yeah you can read the evening "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-8bd473a8556a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtoken2word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'[ME]'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mstra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[ME]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mstra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'[ME] '\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotalstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotalstr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstra\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stra = input('[ME]')\n",
    "stra = '[ME] '+stra\n",
    "totalstr = stra+' '\n",
    "tokens = tokenize_string(stra)\n",
    "for _ in range(200):\n",
    "    y_pred = model.predict(convert_to_tensor([tokens]))\n",
    "    y_pred = y_pred[0][-1]\n",
    "    y_pred = np.argsort(y_pred)[::-1]\n",
    "    if token2word[y_pred[0]] == '[ME]':\n",
    "        stra = input('[ME]')\n",
    "        stra = '[ME] '+stra\n",
    "        totalstr = totalstr+stra+' '\n",
    "        tokens = tokenize_string(totalstr)\n",
    "    else:\n",
    "        if token2word[y_pred[0]] == '[THEM]':\n",
    "            print()\n",
    "        print(token2word[y_pred[0]], end=' ')\n",
    "        totalstr = totalstr+token2word[y_pred[0]]+' '\n",
    "        tokens = np.append(tokens, y_pred[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
