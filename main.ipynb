{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "physical_devices = tensorflow.config.list_physical_devices('GPU')\n",
    "tensorflow.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "DATA = 'input/'\n",
    "INTERMEDIATES = 'intermediates/'\n",
    "MY_NAME = 'Aayush Fadia'\n",
    "OUTFILE='all_chats.txt'\n",
    "MODEL_SAVENAME='two_layer_gru'\n",
    "print(os.listdir('.'))\n",
    "if DATA[:-1] not in os.listdir('./'):\n",
    "    os.mkdir(DATA)\n",
    "if INTERMEDIATES[:-1] not in os.listdir('./'):\n",
    "    os.mkdir(INTERMEDIATES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = str.maketrans('', '', string.punctuation)\n",
    "chats = os.listdir(DATA)\n",
    "def preproocess_message(msg):\n",
    "    msg = re.sub(r'(\\w)\\1+',r'\\1', msg)\n",
    "    msg = re.sub('\\d\\w+', '#', msg)\n",
    "    msg = re.sub('\\d', '#', msg)\n",
    "    msg = re.sub('this mesage was deleted', '[DELETED_MESSAGE]', msg)\n",
    "    msg = re.sub('media omited', '[MEDIA]', msg)\n",
    "    msg = msg.encode('ascii', 'ignore').decode('ascii').strip()+' '\n",
    "    return msg\n",
    "with open(INTERMEDIATES+OUTFILE, 'w') as outfile:\n",
    "    for chat in chats:\n",
    "        outfile.write('[SOC] ')\n",
    "        with open(DATA+chat, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                try:\n",
    "                    msgr = line.split(':')[1].split('-')[1].strip()\n",
    "                    sender_token = '[ME] ' if msgr==MY_NAME else '[THEM] '\n",
    "                    msg = line.split(':')[2].strip().lower().translate(translator)\n",
    "                    msg = preproocess_message(msg)\n",
    "                    outfile.write(sender_token+msg)\n",
    "                except IndexError:\n",
    "                    pass\n",
    "        outfile.write('[EOC] ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chats_file = open(INTERMEDIATES+OUTFILE, 'r')\n",
    "all_chats = all_chats_file.readlines()[0]\n",
    "all_chats = re.sub(' +', ' ',all_chats)\n",
    "all_chats_words = all_chats.split(' ')\n",
    "print(\"Number of tokens: \"+str(len(all_chats_words)))\n",
    "vocab = list(set(all_chats_words))\n",
    "print(\"Number of Distinct tokens: \"+str(len(vocab)))\n",
    "totalsize = len(vocab)\n",
    "token2word = dict()\n",
    "word2token = dict()\n",
    "for i in range(len(vocab)):\n",
    "    token2word[i] = vocab[i]\n",
    "    word2token[vocab[i]] = i\n",
    "all_chats_tokens = [word2token[word] for word in all_chats_words]\n",
    "all_chats_tokens_np = np.asarray(all_chats_tokens, np.uint16)\n",
    "del all_chats_words\n",
    "all_chats_file.close()\n",
    "del all_chats\n",
    "del vocab\n",
    "del all_chats_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = tensorflow.data.Dataset.from_tensor_slices(all_chats_tokens_np)\n",
    "sequences = full_dataset.batch(33, drop_remainder=True)\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "BATCH_SIZE = 256\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU, Input\n",
    "tf.backend.clear_session()\n",
    "def loss(labels, logits):\n",
    "  return tf.losses.sparse_categorical_crossentropy(labels, logits)\n",
    "model = Sequential()\n",
    "model.add(Embedding(totalsize, 256))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(GRU(256, return_sequences=True))\n",
    "model.add(GRU(128, return_sequences=True))\n",
    "model.add(Dense(totalsize, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(loss=loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.backend.clear_session()\n",
    "model = tf.models.load_model('checkpoints/'+MODEL_SAVENAME+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging_callback = tf.callbacks.TensorBoard(log_dir='logs/'+MODEL_SAVENAME, update_freq=1000, profile_batch=0)\n",
    "checkpoint_callback = tf.callbacks.ModelCheckpoint('checkpoints/'+MODEL_SAVENAME+'.h5', save_freq=200000)\n",
    "while True:\n",
    "    try:\n",
    "        model.fit(dataset, epochs=10000, callbacks=[logging_callback, checkpoint_callback], verbose=False)\n",
    "    except KeyboardInterrupt:\n",
    "        model.save(MODEL_SAVENAME+'intrpd.h5')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from tensorflow import get_logger\n",
    "get_logger().setLevel(logging.ERROR)\n",
    "from tensorflow import convert_to_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = dataset.take(1)\n",
    "for x, _ in ip:\n",
    "    x = x[0]\n",
    "    for tkn in x:\n",
    "        tkn = tkn.numpy()\n",
    "        word = token2word[tkn]\n",
    "        if word in ['[THEM]', '[ME]']:\n",
    "            print()\n",
    "        print(word, end=' ')\n",
    "    print('!!GENERATION BEGINS!!')\n",
    "    print()\n",
    "    for _ in range(200):\n",
    "        y_pred = model.predict(convert_to_tensor(np.asarray([x])))\n",
    "        y_pred = y_pred[0][-1]\n",
    "        y_pred = np.argsort(y_pred)[::-1]\n",
    "        if token2word[y_pred[0]] in ['[THEM]', '[ME]']:\n",
    "            print()\n",
    "        print(token2word[y_pred[0]], end=' ')    \n",
    "        x = np.append(x, y_pred[0])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_string(msg):\n",
    "    msg = preproocess_message(msg).strip()\n",
    "    tokens = []\n",
    "    for word in msg.split(' '):\n",
    "        try:\n",
    "            tokens.append(word2token[word])\n",
    "        except KeyError:\n",
    "            print(word+' is out of vocabulary')\n",
    "    return np.asarray(tokens, dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stra = input('[ME]')\n",
    "stra = '[ME] '+stra\n",
    "totalstr = stra+' '\n",
    "tokens = tokenize_string(stra)\n",
    "for _ in range(200):\n",
    "    y_pred = model.predict(convert_to_tensor([tokens]))\n",
    "    y_pred = y_pred[0][-1]\n",
    "    y_pred = np.argsort(y_pred)[::-1]\n",
    "    if token2word[y_pred[0]] == '[ME]':\n",
    "        stra = input('[ME]')\n",
    "        stra = '[ME] '+stra\n",
    "        totalstr = totalstr+stra+' '\n",
    "        tokens = tokenize_string(totalstr)\n",
    "    else:\n",
    "        if token2word[y_pred[0]] == '[THEM]':\n",
    "            print()\n",
    "        print(token2word[y_pred[0]], end=' ')\n",
    "        totalstr = totalstr+token2word[y_pred[0]]+' '\n",
    "        tokens = np.append(tokens, y_pred[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
